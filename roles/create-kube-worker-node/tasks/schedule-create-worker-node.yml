---

# file: schedule-create-worker-node.yml
#

# func memo:
#
#  scheduler routine:
#    . check enviroment status and define if need to deploy
#  
#    . create user/group
#    . create directory (home, bin, cert, config, plugins)
#      ( /etc/kubernetes, /etc/kubernetes/cert and ? kube config directory)
#    . create and copy cert
#    . create etcdctl, kubectl binary
#    . create etcdctl, kubectl run-time rnv
#    . call deploy cni
#      (
#         create cni directory
#         create binary
#         ? other settins
#      )
#    . call deploy flannel
#      ( create bin/config/data dir
#        create cert
#        create binary
#        create systemd unit file
#          ( and setting create docker env config for docker in sevie unit file
#            setting delete docker0 network if exist
#          )
#        firewall setting
#        start flannel service
#      )
#      . call deplaoy docker
#      (
#         create ? user/group
#         create directory (config, data)
#         create binary
#         create systemd unit file (? set docker mirror url)
#         firewall setting
#         start docker service
#      )
#      . call deploy kubelet
#      (
#         create bootstrap token
#         create rbac role for bootstrap csr, auto approve, renew
#         create kubelet systemd unit file
#         create kubelet config file
#         firewall settings
#         start kubelet service
#      )
#      . call deploy coredns
#  

#########################################################################################################

- name: traceing - start schedule-create-worker-node
  debug:
    msg: "check tracing - starting schedule-create-worker-node ..."




#########################################################################################################
# 0.0
# check environmental status and
#   determine if machine we can deploy on it
#
# ( check each node,if /etc/kubernetes directory exists, then avoid deploy on this host )
#

# check kubernetes directory (/etc/kubernetes/ on every host ) exist
- name: check kubernetes directory exist
  stat:
    path: "{{ kube_config_path }}/"
  register: kube_config_path_exist_result


# skip following block if exist condition true
# 0.1
#
#   (using when condition for following block)


##########################################################################################################

#############
# block start
#############
# set skip deploy to  false when kube config directory ( /etc/kubernetes )  not exist
- block:

# 0.
    - name: exec condition promp of worker node
      debug:
        msg: "{{ kube_config_path }} directory existence is {{ kube_config_path_exist_result.stat.exists }}, schedule starting to create kube worker node on {{ inventory_hostname }}"


##########################################################################################################
# . basic environment - user, directories, system cert files, kubernetes binary files, 
#                       etcdctl client, kubectl client, worker node server and client cert creation
#

# 1.
#   create etcd user and group (as etcd client on kuber worker node), also create etcd user
#     home directory (use etcd data dir path (/var/lib/etcd/) as home dir path)
#     (etcd server store etcd db only, client machine do not store etcd db)
    - name: exec create-kube-etcd-user-group on worker node
      import_tasks: ../../create-kube-cluster-master/tasks/kube-etcdctl/create-kube-etcd-user-group.yml
# 2.
#   create kube user and group, create kube user home directory ($HOME/.kube/) and
#     data directory (/var/lib/kube/), and also add kube user to etcd group
    - name: exec create-kube-user-group on worker node
      import_tasks: ../../create-kube-cluster-master/tasks/kube-env/create-kube-user-group.yml
# 3.
#   create kube directories:
#     kube config directory (/etc/kubenetes/)
#     kube cert directory (/etc/kubernetes/cert/)
#     kube manifests directory for static pods (/etc/kubernetes/manifests/)
#     kube log directory (/var/log/kubernetes/)
    - name: create kube directory on worker node
      import_tasks: ../../create-kube-cluster-master/tasks/kube-env/create-kube-directories.yml
# 4.
#   create etcdctl client and enviroment 
#     . create etcd config and cert directory
#     . copy etcd-ca and client cert
#     . copy etcdctl binary
#     . setting etcdctl client environment
    - name: create kube etcd client system and enviroment on worker node
      import_tasks: ../../create-kube-cluster-master/tasks/kube-etcdctl/create-kube-etcd-client.yml
# 5.
#   create kube cert files for worker node
    - name: create kube cert files on worker node
      import_tasks: kube-env/create-kube-worker-node-cert-files.yml
# 6.
#   create kube node binary on worker node
    - name: create kubernetes binary files on worker node
      import_tasks: kube-env/create-kube-worker-node-binary-files.yml
# 7.
#    create kubectl config and cert on worker node
    - name: create kubectl config enviroment
      import_tasks: kube-kubectl/create-kube-worker-node-kubectl-config.yml
# 8.
#    create kube aggregator client config on worker node
    - name: create kube aggregator client config file on worker node
      import_tasks: kube-aggregator-client/create-kube-worker-node-aggregator-client-config.yml
# 9.
    - name: switch systemd-journald (for storage and node-problem-detector daemonset and daemon)
      import_tasks: ../../create-kube-cluster-master/tasks/kube-misc/switch-journal.yml


#######################################################################################################
# . cni container networking interface environment
#

# 10.
#   deploy cni environment on worker node
    - name: deploy cni
      import_tasks: ../../create-kube-cluster-master/tasks/kube-networking/cni/create-cni-environment.yml




#######################################################################################################
# . docker environment and docker service deployment
#

# 11.
#   deploy docker environment on worker node
    - name: deploy docker (but not start it)
      import_tasks: ../../create-kube-cluster-master/tasks/kube-container/docker/create-docker-environment.yml
# 12.
#   enable and start docker service on worker node
    - name: enable and start docker service
      import_tasks: ../../create-kube-cluster-master/tasks/kube-container/docker/enable-and-start-docker-service.yml




#######################################################################################################
# . networking environment for networking components daemonset
#     - optional calico networking environment (not use now. it is replace by kube-router)
#     - kube-router networking environment
#

# 20.
#   created calico common environment on work node
#   remark:
#     as calico node daemonset already deployed but calico common runtime environment not
#     yet settled, this will cause calico node restart for a period untile calico common
#     enviroment created here
#  (check: how to process calico daemonset when we need to add some worker nodes)
#    - name: created calico networking common environment on worker node
#      import_tasks: ../../create-kube-cluster-master/tasks/kube-networking/calico/create-calico-common-environment.yml

# 20. ALTERNATIVE - KUBE-ROUTER NETWORKING
#   created kube-router environment on work node - needed by kube-router daemon deployed on every
#     worker node ( deployed by kube-router daemonset )
#   remark:
#     as kube-router daemonset already deployed but kube-router common runtime environment not
#     yet settled on worker node in deploying, this will cause kube-router pod restart for a period
#     untile kube-router common enviroment created now
#  (check: how to process kube-router daemonset when we need to add some worker nodes)
    - name: created kube-router networking common environment on worker node
      import_tasks: ../../create-kube-cluster-master/tasks/kube-networking/kube-router/create-kube-router-common-environment.yml




########################################################################################################
# . firewall - not use now. replaced by system level iptables service
#

# xx.
# FIREWALL SETTING IS NOW APPLYING IN SYSTEM LEVEL BY USING IPTABLES.SERVICE
#   enable firewall rules on worker node
#    - name: eanble firewall rules on worker node
#      import_tasks: kube-firewall/enable-firewall-rule-for-kube-worker-node.yml




########################################################################################################
# . kubernetes component for kube cluster worker node
#     - kubelet service
#     - kube-proxy service (not use now. it is replaced by kube-router)
#

# 30.
#   deploy kubelet service on worker node
    - name: deploy kubelet service
      import_tasks: kube-core-service/kubelet/deploy-kubelet-service.yml
# 31.
#   deploy post procee for kubelet on worker node
#    - name: deploy kubelet service post process
#      import_tasks: kube-core-service/kubelet/deploy-kubelet-post-process.yml
# ###
# change into using kube-router networking and also using kube-router as cluster service proxy
#   and thus, kube-proxy is not used (replaced by using kube-router)
#   (we create kube-proxy enviroment, but not enable and not start kube-proxy service)
# 32.
#   deploy kube-proxy service on worker node
#    - name: deploy kube-proxy service
#      import_tasks: ../../create-kube-cluster-master/tasks/kube-core-service/proxy/deploy-kube-proxy-service.yml




#########################################################################################################
# . kubernetes worker node bootstrap facility 
#   remark: not use kubernetes worker node bootstrap and renewal facility now. it is replaced by 
#           creating worker node kubelet certification during deploying worker node, and use the
#           certification to certificate communication between apiserver and kubelet.
#

# 40.
# - not need now, and can be deleted. we do not use bootstrap facility, instead of using 
#     certification created by every worker node itself
#   create updated bootstrap token secrect file and update bootstrap token secret
#    - name: create and update bootstrap token secret
#      import_tasks: kube-security/create-update-bootstrap-token-secret.yml




#########################################################################################################
# . test pod or daemonset
#     - hostnames pod - return hostname of pod
#     - my-ip pod     - return host name and ip of pod 
#     - busybox pod - basic busybox exec environment
#     - nginx pod - nginx test server to check service ip, pod ip, node-port
#

# 51.
#   deploy 3 hostnames test pods
    - name: deploy 3 test hostnames pods
      import_tasks: pod-deployment/deploy-hostnames-pod.yml
# 52.
#   deploy 3 test nginx pods
    - name: deploy 3 test nginx pods
      import_tasks: pod-deployment/deploy-nginx-test-pod.yml
# 53.
#   deploy a test busybox (with curl) pod
    - name: deploy a test busybox pod
      import_tasks: pod-deployment/deploy-busybox-pod.yml    
# 54.
#   deploy 3 myip test pods
    - name: deploy 3 myip test pods
      import_tasks: pod-deployment/deploy-myip-test-pod.yml
# 54.
#   deploy 3 hello test pods
    - name: deploy 3 hello test pods
      import_tasks: pod-deployment/deploy-hello-pod.yml



#########################################################################################################
# . misc
#

# 60.
#   setting and enable npt client on worker node
#    - name: enable ntp server and client on node-0 of masters to act as time sync source
#      import_tasks: kube-misc/enable-worker-node-ntp-time-source.yml




#########################################################################################################
# . firewall (not use now. it is replaced by system level iptables service)
#

# 70.
#   enable firewall rules on worker node
#    - name: eanble firewall rules on worker node
#      import_tasks: kube-firewall/enable-firewall-rule-for-kube-worker-node.yml



########################################################################################################
# . final processes
#

# 80.
#   take final process (apply firewall rules, save iptables backup, etc.)
#    - name: take final process
#      import_tasks: kube-post-process/take-final-process.yml




########################################################################################################
# . system default iptables file and facility to make machine enter determined iptables firewall state
#   after reboot
#

# 90.
#   create default iptables rules file for iptables service (in dir /etc/sysconfig/iptables file)
    - name: create default iptables rules file for iptables service
      import_tasks: kube-firewall/create-default-iptables-file-worker-node.yml

# A0.
#   deploy bootup iptables setting setting to get workable iptable/ipvs rules after reboot
    - name: deploy bootup iptables setting service
      import_tasks: kube-firewall/deploy-bootup-iptables-setting-service.yml



#######################################################################################################
#
    
# - condition of block:
#     when for block (exec block when condition true - need create kube cluster master node)
#   (add feateure - deploy worker node on masters)
  when: ( ( kube_config_path_exist_result.stat.exists == false ) or ( inventory_hostname in groups['kube-masters'] ) )

############
# block end
############

# continue do nothing
#   (
- name: deploy condition fail - skip deploying this kube worker node
  debug:
    msg: "WARNING: EXIT deploy kube worker node on {{ inventory_hostname }}. {{ kube_config_path }} exists on {{ inventory_hostname }}, skip deploying kube worker node on {{ inventory_hostname }}."
  when:  ( kube_config_path_exist_result.stat.exists == true ) and (not ( inventory_hostname in groups['kube-masters'] ) )
#   )

